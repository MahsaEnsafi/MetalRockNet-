# -*- coding: utf-8 -*-
"""MetalRockNet.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/16ehLyVztUrLkNr2J-6DKI9A3ZxKo84ER
"""
#Mount Google Drive to access the dataset
from google.colab import drive
drive.mount('/content/drive')
#--------------------------------------------------------------------------------

# Import necessary libraries
import pandas as pd
import numpy as np
from matplotlib import pylab as plt
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix,classification_report
import tensorflow as tf
from tensorflow.keras import models,layers,losses
from sklearn.metrics import confusion_matrix,classification_report
import seaborn as sn
#---------------------------------------------------------------------------------

# Load the dataset from CSV file
data=pd.read_csv('/path/data.csv')
print(data.sample(4))  # Display a random sample of 4 rows from the dataset
#------------------------------------------------------------------------------------

# Print the shape of the dataset
print(data.shape)
#-------------------------------------------------------------------------------------

# Print the column names in the dataset
print(data.columns)
#--------------------------------------------------------------------------------------

# Display the distribution of target variable 'Column61'
print(data['Column61'].value_counts())
#-------------------------------------------------------------------------------------

# Prepare features (X) and target (Y) for modeling
X=data.drop('Column61',axis=1) # Drop target column from features
Y=data['Column61']  # Target variable
print(X.shape) # Print shape of features
print(Y.shape) # Print shape of target
#---------------------------------------------------------------------------------------

# Convert categorical target variable to dummy/indicator variables
Y = pd.get_dummies(Y, drop_first=True, dtype=np.float32)  # R:1 M:0
print(Y.head())  # Display the first few rows of the target variable
#--------------------------------------------------------------------------------------

# Check the value counts of the target variable
print(Y.value_counts())
#-------------------------------------------------------------------------------------------

# Split the dataset into training and testing sets
X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.25, random_state=5)
print(X_train.shape)  # Print shape of training features
print(Y_train.shape)  # Print shape of training target
print(X_test.shape)  # Print shape of testing features
print(Y_test.shape)  # Print shape of testing target
#-------------------------------------------------------------------------------------------

#creating and training model
def my_model():
  model=models.Sequential([
      layers.Dense(60,input_shape=(60,),activation='relu'),
      layers.Dense(100,activation='relu'),
      layers.Dropout(0.5),
      layers.Dense(75,activation='relu'),
      layers.Dropout(0.5),
      layers.Dense(50,activation='relu'),  
      layers.Dropout(0.5),
      layers.Dense(25,activation='relu'),
      layers.Dense(1,activation='sigmoid')
  ])
  model.compile(
      optimizer='adam',
      loss=losses.BinaryCrossentropy,
      metrics=['accuracy','precision','recall','f1_score']
  )
  return model

def eval(model,X,Y):
  model.evaluate(X,Y)

  #prediction
def prediction(model,X,Y):
  Y_predict=model.predict(X)
  Y_pred=[]
  for label in Y_predict:
    if label<0.5:
      Y_pred.append(0)
    else:
      Y_pred.append(1)
  print('predicted the five first labels:',Y_pred[:5])
  print('True label of first five elements:',Y[:5])
  return Y_pred


  #show classification report
def report(truth,predictions):
  print(classification_report(truth,predictions))
  #show confusion matrix
  cm=tf.math.confusion_matrix(labels=truth,predictions=predictions)
  plt.figure(figsize=(10,7))
  sn.heatmap(cm,annot=True,fmt='d')
  plt.xlabel('predicted')
  plt.ylabel('Truth')

def plot_loss(history):
    plt.figure(figsize=(10, 6))
    plt.plot(history.history['loss'], label='Train Loss')
    plt.plot(history.history['val_loss'], label='Validation Loss')
    plt.title('Training and Validation Loss')
    plt.xlabel('Epochs')
    plt.ylabel('Loss')
    plt.legend()
    plt.grid()
    plt.show()

def plot_accuracy(history):
    plt.figure(figsize=(10, 6))
    plt.plot(history.history['accuracy'], label='Train accuracy')
    plt.plot(history.history['val_accuracy'], label='Validation accuracy')
    plt.title('Training and Validation accuracy')
    plt.xlabel('Epochs')
    plt.ylabel('accuracy')
    plt.legend()
    plt.grid()
    plt.show()

#-------------------------------------------------------------------------------------------------------------------
# Create and train the model
model=my_model() # Instantiate the model
history=model.fit(X_train,Y_train,epochs=100,validation_split=0.2,batch_size=8) # Instantiate the model
#--------------------------------------------------------------------------------------------------------------------

# Plot training and validation loss and accuracy
plot_loss(history)
plot_accuracy(history)
#---------------------------------------------------------------------------------------------------------------

# Evaluate the model on the test set
eval(model,X_test,Y_test)
#--------------------------------------------------------------------------------------------------------------------

# Make predictions
preds=prediction(model,X_test,Y_test)
#---------------------------------------------------------------------------------------------------------------

#Generate a report
report(Y_test,preds)
